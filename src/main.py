"""
SONAR POKOJOWY - GÅ‚Ã³wny agent
Koordynuje: scraping â†’ parsowanie â†’ geokodowanie â†’ wykrywanie duplikatÃ³w â†’ zapis
WERSJA 2.0: RÃ³wnolegÅ‚y scraping + monitoring
"""

import json
from pathlib import Path
from datetime import datetime, timedelta
import pytz
from typing import List, Dict
import time

# Import lokalnych moduÅ‚Ã³w
from scraper import OLXScraper
from address_parser import AddressParser
from price_parser import PriceParser
from geocoder import Geocoder
from duplicate_detector import DuplicateDetector
from scan_logger import ScanLogger

class SonarPokojowy:
    def __init__(self, data_file: str = "../data/offers.json", removed_file: str = "../data/removed_listings.json"):
        self.data_file = Path(data_file)
        self.removed_file = Path(removed_file)
        self.address_parser = AddressParser()
        self.price_parser = PriceParser()
        self.geocoder = Geocoder(cache_file="../data/geocoding_cache.json")
        self.duplicate_detector = DuplicateDetector(similarity_threshold=0.95)
        self.scan_logger = ScanLogger(log_file="../data/scan_history.json")
        
        # Strefa czasowa polska
        self.tz = pytz.timezone('Europe/Warsaw')
        
        # Wczytaj istniejÄ…cÄ… bazÄ™
        self.database = self._load_database()
        
        # Wczytaj listÄ™ usuniÄ™tych ogÅ‚oszeÅ„
        self.removed_listings = self._load_removed_listings()
        
        # Inicjalizuj scraper Z istniejÄ…cymi ofertami (inteligentne pomijanie)
        existing_offers = self._build_existing_offers_index()
        self.scraper = OLXScraper(delay_range=(0.5, 1), max_workers=5, existing_offers=existing_offers)
    
    def _build_existing_offers_index(self) -> Dict:
        """
        Buduje indeks istniejÄ…cych ofert dla inteligentnego pomijania.
        Returns: {offer_id: {'price': X, 'description': '...'}}
        """
        index = {}
        for offer in self.database.get('offers', []):
            if offer.get('active', False):  # Tylko aktywne oferty
                index[offer['id']] = {
                    'price': offer.get('price', {}).get('current'),
                    'description': offer.get('description', ''),
                    'previous_price': offer.get('price', {}).get('previous_price')
                }
        print(f"ğŸ“š Zaindeksowano {len(index)} aktywnych ofert do inteligentnego pomijania")
        return index
    
    def _load_database(self) -> Dict:
        """Wczytuje bazÄ™ danych z JSON."""
        if self.data_file.exists():
            try:
                with open(self.data_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except json.JSONDecodeError:
                print("âš ï¸ Uszkodzony plik bazy danych, tworzÄ™ nowy")
                return self._create_empty_database()
        else:
            return self._create_empty_database()
    
    def _load_removed_listings(self) -> set:
        """Wczytuje listÄ™ usuniÄ™tych ogÅ‚oszeÅ„."""
        if self.removed_file.exists():
            try:
                with open(self.removed_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return set(data.get('removed_ids', []))
            except json.JSONDecodeError:
                print("âš ï¸ Uszkodzony plik usuniÄ™tych ogÅ‚oszeÅ„, tworzÄ™ nowy")
                return set()
        else:
            return set()
    
    def _save_removed_listings(self):
        """Zapisuje listÄ™ usuniÄ™tych ogÅ‚oszeÅ„."""
        self.removed_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.removed_file, 'w', encoding='utf-8') as f:
            json.dump({
                'removed_ids': list(self.removed_listings),
                'last_updated': datetime.now(self.tz).isoformat()
            }, f, ensure_ascii=False, indent=2)
    
    def _create_empty_database(self) -> Dict:
        """Tworzy pustÄ… strukturÄ™ bazy danych."""
        return {
            "last_scan": None,
            "next_scan": None,
            "offers": []
        }
    
    def _save_database(self):
        """Zapisuje bazÄ™ danych do JSON."""
        self.data_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.data_file, 'w', encoding='utf-8') as f:
            json.dump(self.database, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ Baza zapisana: {self.data_file}")
    
    def _calculate_next_scan_time(self) -> str:
        """Oblicza czas nastÄ™pnego scanu (9:00, 15:00 lub 21:00)."""
        now = datetime.now(self.tz)
        scan_hours = [9, 15, 21]
        
        for hour in scan_hours:
            next_time = now.replace(hour=hour, minute=0, second=0, microsecond=0)
            if next_time > now:
                return next_time.isoformat()
        
        # JeÅ›li po 21:00, to nastÄ™pny scan o 9:00 nastÄ™pnego dnia
        tomorrow = now + timedelta(days=1)
        next_time = tomorrow.replace(hour=9, minute=0, second=0, microsecond=0)
        return next_time.isoformat()
    
    def _process_offer(self, raw_offer: Dict) -> Dict:
        """
        Przetwarza surowe ogÅ‚oszenie: parsuje adres, cenÄ™, geokoduje.
        
        Returns:
            Dict z przetworzonymi danymi lub None jeÅ›li oferta nieprawidÅ‚owa
        """
        # 1. UÅ¼yj peÅ‚nego opisu (scraper juÅ¼ go pobraÅ‚)
        full_text = raw_offer['title'] + " " + raw_offer.get('description', '')
        
        # FILTR: Wykluczamy ogÅ‚oszenia ktÃ³re nie sÄ… pokojami w mieszkaniach
        excluded_phrases = [
            'dom jednorodzinny',
            'w domu jednorodzinnym',
            'domek jednorodzinny',
            'willa',
            'domek',
            'dom w zabudowie',
            'segment',
            'bliÅºniak'
        ]
        
        full_text_lower = full_text.lower()
        for phrase in excluded_phrases:
            if phrase in full_text_lower:
                print(f"      âš ï¸ Wykluczono: {phrase}")
                return None
        
        # 2. Parsuj adres z peÅ‚nego tekstu (tytuÅ‚ + opis)
        address_data = self.address_parser.extract_address(full_text)
        
        # JeÅ›li nie znaleziono adresu w tytule, sprÃ³buj w samym opisie
        if not address_data and raw_offer.get('description'):
            print(f"      ğŸ” Brak adresu w tytule, szukam w opisie...")
            address_data = self.address_parser.extract_address(raw_offer['description'])
        
        if not address_data:
            return None  # Brak adresu â†’ ignoruj
        
        # 3. Parsuj cenÄ™ - NOWA LOGIKA TRÃ“JPOZIOMOWA (2C)
        # PRIORYTET 1: JSON-LD z OLX (najbardziej niezawodne, oficjalne dane)
        # PRIORYTET 2: Cache (dane z poprzedniego skanu - rÃ³wnie niezawodne jak JSON-LD)
        # PRIORYTET 3: Parser ceny z treÅ›ci (wyciÄ…ga czystÄ… cenÄ™ pokoju bez mediÃ³w)
        # PRIORYTET 4: Fallback HTML (jeÅ›li JSON-LD i parser zawiodÅ‚y)
        
        price = None
        media_info = "brak informacji"
        price_source = None
        
        # SprawdÅº czy mamy JSON-LD z niezawodnÄ… cenÄ…
        if raw_offer.get('official_price') and raw_offer.get('price_source') == 'json-ld':
            # PRIORYTET 1: JSON-LD - najbardziej niezawodne ÅºrÃ³dÅ‚o
            price = raw_offer['official_price']
            price_source = "JSON-LD (OLX)"
            
            # Wykryj info o mediach uÅ¼ywajÄ…c parsera (BEZ parsowania ceny!)
            media_info = self.price_parser.detect_media_info_only(full_text)
            
            print(f"      ğŸ’° UÅ¼yto ceny JSON-LD: {price} zÅ‚ ({media_info})")
        
        # PRIORYTET 2: Cache - dane z poprzedniego skanu (rÃ³wnie niezawodne)
        elif raw_offer.get('official_price') and raw_offer.get('price_source') == 'cache':
            # Cache - oferta pominiÄ™ta w scraping bo cena siÄ™ nie zmieniÅ‚a
            price = raw_offer['official_price']
            price_source = "cache"
            
            # Wykryj info o mediach uÅ¼ywajÄ…c parsera (BEZ parsowania ceny!)
            media_info = self.price_parser.detect_media_info_only(full_text)
            
            print(f"      ğŸ’° UÅ¼yto ceny z cache (pominiÄ™to pobieranie): {price} zÅ‚ ({media_info})")
        
        # PRIORYTET 3: Parser tekstowy - wyciÄ…ga czystÄ… cenÄ™ pokoju
        if not price:
            price_data = self.price_parser.extract_price(full_text)
            if price_data:
                price = price_data['price']
                media_info = price_data['media_info']
                price_source = "Parser tekstowy"
                print(f"      ğŸ’° UÅ¼yto parsera ceny z opisu: {price} zÅ‚ ({media_info})")
        
        # PRIORYTET 4: Fallback - cena z HTML (jeÅ›li JSON-LD i parser zawiodÅ‚y)
        if not price and raw_offer.get('official_price'):
            price = raw_offer['official_price']
            media_info = self.price_parser.detect_media_info_only(full_text)
            price_source = "HTML fallback"
            print(f"      ğŸ’° UÅ¼yto ceny HTML (fallback): {price} zÅ‚ ({media_info})")
        
        if not price:
            return None  # Brak ceny â†’ ignoruj
        
        # 4. Geokoduj adres
        coords = self.geocoder.geocode_address(address_data['full'])
        if not coords:
            print(f"âš ï¸ Nie moÅ¼na geokodowaÄ‡: {address_data['full']}")
            return None  # Nie znaleziono wspÃ³Å‚rzÄ™dnych â†’ ignoruj
        
        # 5. StwÃ³rz ID z URL (unikalne)
        offer_id = raw_offer['url'].split('/')[-1].split('.')[0]
        
        return {
            'id': offer_id,
            'url': raw_offer['url'],
            'address': {
                'full': address_data['full'],
                'street': address_data['street'],
                'number': address_data['number'],
                'coords': coords
            },
            'price': {
                'current': price,
                'history': [price],
                'media_info': media_info,
                'source': price_source  # Dodane: JSON-LD / Parser / HTML fallback
            },
            'description': full_text,
            'first_seen': datetime.now(self.tz).isoformat(),
            'last_seen': datetime.now(self.tz).isoformat(),
            'active': True,
            'days_active': 0
        }
    
    def _find_existing_offer(self, offer_id: str) -> Dict:
        """Znajduje istniejÄ…ce ogÅ‚oszenie po ID."""
        for offer in self.database['offers']:
            if offer['id'] == offer_id:
                return offer
        return None
    
    def _update_existing_offer(self, existing: Dict, new_data: Dict):
        """Aktualizuje istniejÄ…ce ogÅ‚oszenie z inteligentnym zarzÄ…dzaniem cenÄ…."""
        now = datetime.now(self.tz).isoformat()
        
        # Aktualizuj last_seen
        existing['last_seen'] = now
        
        # INTELIGENTNA AKTUALIZACJA CENY - priorytetyzuj ÅºrÃ³dÅ‚a
        old_price = existing['price']['current']
        new_price = new_data['price']['current']
        old_source = existing['price'].get('source', 'unknown')
        new_source = new_data['price'].get('source', 'unknown')
        
        # Hierarchia ÅºrÃ³deÅ‚ (od najlepszego do najgorszego)
        source_priority = {
            'JSON-LD (OLX)': 3,
            'cache': 3,  # Cache ma ten sam priorytet co JSON-LD (bo pochodzi z niego)
            'HTML fallback': 2,
            'Parser tekstowy': 1,
            'unknown': 0
        }
        
        old_priority = source_priority.get(old_source, 0)
        new_priority = source_priority.get(new_source, 0)
        
        # SZCZEGÃ“ÅOWE LOGOWANIE ZMIAN CEN
        print(f"      ğŸ” Analiza ceny dla oferty: {existing['id']}")
        print(f"         Stara cena: {old_price} zÅ‚ (ÅºrÃ³dÅ‚o: {old_source}, priorytet: {old_priority})")
        print(f"         Nowa cena: {new_price} zÅ‚ (ÅºrÃ³dÅ‚o: {new_source}, priorytet: {new_priority})")
        
        # DECYZJA: Aktualizuj cenÄ™ tylko jeÅ›li:
        # 1. Nowe ÅºrÃ³dÅ‚o ma wyÅ¼szy priorytet, LUB
        # 2. Ten sam priorytet ale cena siÄ™ zmieniÅ‚a (realna zmiana ceny), LUB
        # 3. RÃ³Å¼nica ceny jest mniejsza niÅ¼ 50% (zabezpieczenie przed bÅ‚Ä™dami parsera)
        
        should_update = False
        update_reason = None
        
        if new_priority > old_priority:
            # Lepsze ÅºrÃ³dÅ‚o - aktualizuj
            should_update = True
            update_reason = f"Upgrade ÅºrÃ³dÅ‚a: {old_source} â†’ {new_source}"
            print(f"      ğŸ’° {update_reason}")
        elif new_priority == old_priority and old_price != new_price:
            # To samo ÅºrÃ³dÅ‚o ale inna cena - sprawdÅº czy zmiana sensowna
            price_diff_percent = abs(new_price - old_price) / old_price * 100
            
            if price_diff_percent < 50:  # Max 50% zmiany
                should_update = True
                update_reason = f"Zmiana ceny (to samo ÅºrÃ³dÅ‚o): {old_price} â†’ {new_price} zÅ‚ ({price_diff_percent:.1f}%)"
                print(f"      ğŸ’° {update_reason}")
            else:
                # Zbyt duÅ¼a zmiana - podejrzane, nie aktualizuj
                print(f"      âš ï¸ PODEJRZANA zmiana ceny: {old_price} â†’ {new_price} zÅ‚ ({price_diff_percent:.1f}%) - IGNORUJÄ˜")
        elif new_priority < old_priority:
            # Gorsze ÅºrÃ³dÅ‚o - nie aktualizuj
            print(f"      â„¹ï¸ Zachowano cenÄ™ z lepszego ÅºrÃ³dÅ‚a: {old_source} ({old_price} zÅ‚)")
        else:
            # Ta sama cena, to samo ÅºrÃ³dÅ‚o - brak zmian
            print(f"      âœ“ Cena bez zmian: {old_price} zÅ‚")
        
        if should_update and old_price != new_price:
            # NOWE: Zapisz poprzedniÄ… cenÄ™ przed aktualizacjÄ…
            existing['price']['previous_price'] = old_price
            existing['price']['price_changed_at'] = now
            
            # OkreÅ›l kierunek zmiany
            if new_price < old_price:
                existing['price']['price_trend'] = 'down'
                print(f"      ğŸ“‰ Cena SPADÅA: {old_price} â†’ {new_price} zÅ‚ (â†“{old_price - new_price} zÅ‚)")
                print(f"      ğŸ“ PowÃ³d zmiany: {update_reason}")
            else:
                existing['price']['price_trend'] = 'up'
                print(f"      ğŸ“ˆ Cena WZROSÅA: {old_price} â†’ {new_price} zÅ‚ (â†‘{new_price - old_price} zÅ‚)")
                print(f"      ğŸ“ PowÃ³d zmiany: {update_reason}")
            
            existing['price']['current'] = new_price
            existing['price']['source'] = new_source
            
            # Dodaj do historii
            existing['price']['history'].append(new_price)
        
        # Zawsze aktualizuj media_info (moÅ¼e siÄ™ zmieniÄ‡ niezaleÅ¼nie)
        existing['price']['media_info'] = new_data['price']['media_info']
        
        # Upewnij siÄ™ Å¼e jest aktywne
        existing['active'] = True
    
    def _mark_inactive_offers(self, current_offer_ids: List[str]):
        """Oznacza ogÅ‚oszenia jako nieaktywne jeÅ›li nie ma ich w bieÅ¼Ä…cym scanie."""
        for offer in self.database['offers']:
            if offer['id'] not in current_offer_ids and offer['active']:
                offer['active'] = False
                
                # Oblicz dni aktywnoÅ›ci
                first_seen = datetime.fromisoformat(offer['first_seen'])
                last_seen = datetime.fromisoformat(offer['last_seen'])
                offer['days_active'] = (last_seen - first_seen).days
    
    def _cleanup_old_offers(self, max_age_days: int = 548):
        """
        Usuwa oferty starsze niÅ¼ 1.5 roku (548 dni).
        """
        cutoff_date = datetime.now(self.tz) - timedelta(days=max_age_days)
        
        original_count = len(self.database['offers'])
        
        self.database['offers'] = [
            offer for offer in self.database['offers']
            if datetime.fromisoformat(offer['first_seen']) > cutoff_date
        ]
        
        removed = original_count - len(self.database['offers'])
        if removed > 0:
            print(f"ğŸ—‘ï¸ UsuniÄ™to {removed} ofert starszych niÅ¼ 1.5 roku")
    
    def run_scan(self):
        """GÅ‚Ã³wny proces skanowania z logowaniem statystyk."""
        print("\n" + "="*60)
        print("ğŸ¯ SONAR POKOJOWY - Scan Started")
        print("="*60 + "\n")
        
        scan_start_time = time.time()
        now = datetime.now(self.tz)
        print(f"â° Czas: {now.strftime('%Y-%m-%d %H:%M:%S %Z')}\n")
        
        # Rozpocznij logowanie
        self.scan_logger.start_scan()
        
        try:
            # 1. Scraping OLX
            print("ğŸ“¡ Krok 1: Scraping OLX...")
            scraping_start = time.time()
            
            raw_offers = self.scraper.scrape_all_pages(max_pages=20)
            
            scraping_duration = time.time() - scraping_start
            self.scan_logger.log_phase('scraping', scraping_duration, {
                'offers_found': len(raw_offers),
                'max_pages': 20
            })
            
            print(f"âœ… Pobrano {len(raw_offers)} surowych ofert\n")
            
            # 2. Przetwarzanie ofert
            print("ğŸ”§ Krok 2: Przetwarzanie ofert...")
            processing_start = time.time()
            
            processed_offers = []
            skipped_no_address = 0
            skipped_no_price = 0
            skipped_no_coords = 0
            skipped_duplicate = 0
            skipped_removed = 0
            
            for i, raw_offer in enumerate(raw_offers, 1):
                print(f"   [{i}/{len(raw_offers)}] Przetwarzam: {raw_offer['title'][:50]}...")
                
                # StwÃ³rz ID z URL
                offer_id = raw_offer['url'].split('/')[-1].split('.')[0]
                
                # FILTR: PomiÅ„ usuniÄ™te ogÅ‚oszenia
                if offer_id in self.removed_listings:
                    print(f"      ğŸš« PominiÄ™to - ogÅ‚oszenie usuniÄ™te przez uÅ¼ytkownika")
                    skipped_removed += 1
                    continue
                
                processed = self._process_offer(raw_offer)
                
                if not processed:
                    # Zlicz powody odrzucenia
                    full_text = raw_offer['title'] + " " + raw_offer.get('description', '')
                    if not self.address_parser.extract_address(full_text):
                        skipped_no_address += 1
                    elif not self.price_parser.extract_price(full_text) and not raw_offer.get('official_price'):
                        skipped_no_price += 1
                    else:
                        skipped_no_coords += 1
                    continue
                
                # SprawdÅº duplikaty
                if self.duplicate_detector.filter_duplicates(processed, processed_offers):
                    skipped_duplicate += 1
                    print(f"      âš ï¸ Duplikat - ignorujÄ™")
                    continue
                
                processed_offers.append(processed)
                print(f"      âœ… {processed['address']['full']} - {processed['price']['current']} zÅ‚")
            
            processing_duration = time.time() - processing_start
            self.scan_logger.log_phase('processing', processing_duration, {
                'processed': len(processed_offers),
                'skipped_no_address': skipped_no_address,
                'skipped_no_price': skipped_no_price,
                'skipped_no_coords': skipped_no_coords,
                'skipped_duplicate': skipped_duplicate,
                'skipped_removed': skipped_removed
            })
            
            print(f"\nâœ… Przetworzone oferty: {len(processed_offers)}")
            print(f"   PominiÄ™te - brak adresu: {skipped_no_address}")
            print(f"   PominiÄ™te - brak ceny: {skipped_no_price}")
            print(f"   PominiÄ™te - brak wspÃ³Å‚rzÄ™dnych: {skipped_no_coords}")
            print(f"   PominiÄ™te - duplikaty: {skipped_duplicate}")
            print(f"   PominiÄ™te - usuniÄ™te przez uÅ¼ytkownika: {skipped_removed}\n")
            
            # 3. Aktualizacja bazy danych
            print("ğŸ’¾ Krok 3: Aktualizacja bazy danych...")
            
            current_offer_ids = []
            new_offers_count = 0
            updated_offers_count = 0
            
            for processed in processed_offers:
                current_offer_ids.append(processed['id'])
                
                existing = self._find_existing_offer(processed['id'])
                
                if existing:
                    self._update_existing_offer(existing, processed)
                    updated_offers_count += 1
                else:
                    self.database['offers'].append(processed)
                    new_offers_count += 1
            
            # Oznacz nieaktywne
            self._mark_inactive_offers(current_offer_ids)
            
            print(f"   Nowe oferty: {new_offers_count}")
            print(f"   Zaktualizowane: {updated_offers_count}")
            
            # 4. Czyszczenie starych ofert
            print("\nğŸ—‘ï¸ Krok 4: Czyszczenie starych ofert...")
            self._cleanup_old_offers(max_age_days=548)
            
            # 5. Aktualizacja metadanych
            self.database['last_scan'] = now.isoformat()
            self.database['next_scan'] = self._calculate_next_scan_time()
            
            # 6. Zapisz bazÄ™
            print("\nğŸ’¾ Krok 5: Zapisywanie bazy danych...")
            self._save_database()
            
            # 7. Loguj statystyki
            total_duration = time.time() - scan_start_time
            
            active = sum(1 for o in self.database['offers'] if o['active'])
            inactive = len(self.database['offers']) - active
            
            self.scan_logger.log_stats({
                'raw_offers': len(raw_offers),
                'processed': len(processed_offers),
                'new': new_offers_count,
                'updated': updated_offers_count,
                'total_in_db': len(self.database['offers']),
                'active': active,
                'inactive': inactive,
                'skipped_no_address': skipped_no_address,
                'skipped_no_price': skipped_no_price,
                'skipped_no_coords': skipped_no_coords,
                'skipped_duplicate': skipped_duplicate,
                'skipped_removed': skipped_removed
            })
            
            self.scan_logger.end_scan('completed', total_duration)
            
            # 8. Podsumowanie
            print("\n" + "="*60)
            print("ğŸ“Š PODSUMOWANIE SCANU")
            print("="*60)
            print(f"âœ… Oferty aktywne: {active}")
            print(f"ğŸ“ Oferty nieaktywne (historia): {inactive}")
            print(f"ğŸ“¦ ÅÄ…cznie w bazie: {len(self.database['offers'])}")
            print(f"â±ï¸ Czas wykonania: {total_duration:.1f}s")
            print(f"â° NastÄ™pny scan: {datetime.fromisoformat(self.database['next_scan']).strftime('%Y-%m-%d %H:%M')}")
            print("="*60 + "\n")
            
        except Exception as e:
            # W przypadku bÅ‚Ä™du, zaloguj i zakoÅ„cz jako failed
            print(f"\nâŒ BÅ‚Ä…d podczas skanowania: {e}")
            self.scan_logger.log_error(str(e))
            self.scan_logger.end_scan('failed', time.time() - scan_start_time)
            raise


if __name__ == "__main__":
    agent = SonarPokojowy(data_file="../data/offers.json")
    agent.run_scan()
